---
title: "Overview"
sidebar:
  # Set a custom order for the link (lower numbers are displayed higher up)
  order: 1
---
import { Steps } from '@astrojs/starlight/components';
import { Aside } from '@astrojs/starlight/components';


### Introduction

A crucial part of successfully working with RDI is knowing how to build and manage the data pipelines that move data from a source database to Redis. Recall that when we talk about data pipelines, we are simply referring to the process of capturing change data from a source database and transforming the data into compatible Redis data structures for storage in a target Redis database. In RDI these pipelines are called "jobs" and they are defined declaratively with YAML configuration files that require no coding.

RDI allows you to write a job file (a `.yaml` file) for each table from the source database that you want to transform. Additionally, a default job file (`default-job.yaml`) can be used to apply transformations to any tables not explicitly covered by other job files.

For example, you might create a `employee_table.yaml` file to transform only data from an "Employee" table, while also using a `default-job.yaml` to handle transformations for all other tables in the source database.

As part of any job file, RDI will allow you to specify whether you want to move the data into Redis as JSON, hashes, sets, streams, sorted sets, or strings.

When a job runs, this typically involves two separate stages:
<Steps>
1. First, the data ingested during CDC is automatically transformed into JSON.
2. Then, this JSON gets passed on via Redis streams to the stream processor where user-defined transformations (defined inside of the job files) occur.
</Steps>

The diagram below shows the full flow of data for an RDI pipeline highlighting both stages for a single table transformation:

<img class="image-center" src="/doc/assets/pipeline-overview.svg"/>


### Pipeline configuration overview

In order to start working with RDI, its important to know the general structure of how RDI organizes jobs. RDI uses a set of YAML files to configure each pipeline. The following diagram shows the folder structure of the configuration:

<img class="image-center" src="/doc/assets/ingest-config-folders.svg"/>

Notice how RDI hosts a `config.yaml` file. This file serves as the main configuration file for the pipeline. Inside the file, the configuration will specify connection details for the source database and queries that RDI will use to extract the required data from the source database.

One level down, there is a "jobs" folder that is used to store all the job files that will be run on the pipeline. The `default-job.yaml` will be applied to any table that doesn't have its own dedicated `.yaml` job file.

<Aside>
If you want RDI to capture only specific tables while excluding others, you will be able to customize this behavior in the `config.yaml` file. We will take a look at an example in the next section.
</Aside>

Let's now dive into the specifics of configuration and job files. 