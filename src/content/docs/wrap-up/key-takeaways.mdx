---
title: "Key takeaways"
sidebar:
 # Set a custom order for the link (lower numbers are displayed higher up)
 order: 1
---
import { Card } from '@astrojs/starlight/components';
import { Steps } from '@astrojs/starlight/components';

Great job on making it to the end of the RDI Lab. At this point, you should feel comfortable enough to get RDI up and running as well as write and deploy your first pipelines. In this final section, you can review the main takeaways, specific section takeaways, as well as a handy key-term cheatsheet. Take a moment to review the resources, and then when you're finished, visit the "Finish Lab" section which will return you to Redis University. 

### Main takeaways

1. Redis Data Integration (RDI) is a real-time data streaming and synchronization tool that captures changes from source databases and replicates them into Redis. It enables fast data access for caching, event-driven applications, and various other use cases by continuously ingesting and transforming data. RDI operates through Change Data Capture (CDC) mechanisms (via Debezium) and low-code, YAML-based pipelines, making it easy to configure and manage data flows with minimal development effort.
2. RDI can be deployed on either a Virtual Machine (VM) or a Kubernetes-based architecture, both of which support high-availability setups. Regardless of the deployment method, RDI uses the same core components, including a CDC collector and a stream processor. It also requires an RDI metadata database and a target Redis database. In this lab, we set up a VM-based RDI deployment and configured both RDI and the target database using Redis Software.
3. A source database must be prepared for CDC via Debezium. The specific Debezium implementation required depends on the type of source database. For PostgreSQL, this involves enabling logical replication. In this lab, we adjusted the `wal_level` setting in PostgreSQL to enable this functionality.
4. RDI uses YAML-based pipelines to define how data should be moved and transformed. An RDI pipeline consists of a main configuration file (`config.yaml`) and optional job files. One such optional job file is `default-job.yaml`, which applies transformations to any target data not explicitly handled by a specific job file. Within a job file, RDI allows you to specify whether data should be moved into Redis as JSON, hashes, sets, streams, sorted sets, or strings. In this lab, we practiced building and deploying various jobs using Redis Insight.
5. RDI provides various tools and metrics to ensure the deployment runs as expected. These include RDI metric endpoints, logs, and the official Redis support package. Additionally, RDI can be monitored via Redis Insight. In this lab, we set up Prometheus and Grafana to observe our RDI instance and explored common troubleshooting scenarios.


### Section Review

<details>
<summary>RDI basics</summary>
1. Redis Data Integration is a tool built by Redis that lets teams take app data from their existing legacy databases (e.g., Oracle, PSQL) and easily synchronize that data into Redis without using code.
2. RDI captures changes using a process known as Change Data Capture (CDC) using an open-source software called Debezium and by streaming the changes to a Redis database. During the capture process, the data will be transformed into a Redis-compatible data structure. 
</details>
<details>
<summary>RDI architecture</summary>
1. The components in the RDI architecture include:
    - A source database, typically a relational database (e.g., Postgres, MySQL)
    - A collector to perform CDC (Debezium)
    - A RDI metadata database (sometimes also called the "Staging" Database) to process data streams and store RDI metadata
    - A Stream processor that handles transformations on the data RDI captures
    - A Redis database that stores the transformed data
  
   <img class="image-center" src="/doc/assets/arch-step-2.svg" />
2. There are two phases in the RDI pipeline lifecycle: Initial cache loading and change streaming. 
3. At a high level, a RDI pipeline goes through 4 steps to get data from the source to the target:
   <img class="image-center" src="/doc/assets/lifecycle-4.svg" />
4. The RDI control plane has several components that help RDI run:
   <img class="image-center" src="/doc/assets/control-plane.svg" />
5. RDI can be primarily managed via the RDI Cli or Redis Insight:
   <img class="image-center" src="/doc/assets/managment-plane.svg" />
6. RDI can be run in a high-availability setup: 
   <img class="image-center" src="/doc/assets/high-av.svg" />
7. RDI can be run on K8s:
    <img class="image-center" src="/doc/assets/k8-dataplane.png" />
    <img class="image-center" src="/doc/assets/k8-controlplane.png" />
</details>
<details>
<summary>Database prep</summary>
1. Before starting to work with RDI, a source database must be prepped for CDC via Debezium. The Debezium implementation that is needed depends on the type of source database.
</details>
<details>
<summary>Data pipelines</summary>
1. A crucial part of RDI is the data pipelines that power the migration of data from the source database to the target database. In RDI, these data pipelines are called "Jobs" and are written in YAML.
2. RDI Jobs run in two stages. First, the data is ingested by CDC and transformed into JSON. Then, this JSON data gets passed on via streams to the stream processor for further the custom user-defined transformation.
<img class="image-center" src="/doc/assets/pipeline-overview.svg" />
3. A RDI instance hosts a `config.yaml` file (the main configuration file) and several optional job files for specific transformations. RDI follows the following folder structure: 
<img class="image-center" src="/doc/assets/ingest-config-folders.svg" />
4. An RDI instance's main configuration file (`config.yaml`) has two primary properties: sources and targets. 
    - The sources section allows for the configuration of the specific sources to use with RDI
    - The targets section allows for the configuration of specific properties for the target database
5. An RDI configuration file might look like this:

    ```yaml
      targets:
        target:
          connection:
            type: redis
            host: 172.16.22.21
            port: 12000
      sources:
        psql:
          type: cdc
          logging:
            level: info
          connection:
            type: postgresql
            host: 172.16.22.7
            port: 5432
            database: chinook
            user: postgres
            password: postgres
    ```

6. An RDI job file has three primary properties: Source, transform, and output.
    - The source property is a mandatory section that specifies the data items that you want to use
    - The transform property is an optional section describing the transformation that the pipeline applies to the data before writing it to the target database
    - The output property is a required section to specify the data structure(s) that RDI will write to the target database and the text pattern for the key(s) that will access it.

7. An example RDI job might look like this: 

    ```yaml
    source:
      table: Track
    transform:
      - uses: add_field
        with:
          field: NameUpper
          expression: upper("Name")
          language: sql
      - uses: filter
        with:
          expression: GenreId = 1
          language: sql
    output:
      - uses: redis.write
        with:
          data_type: hash
          key:
            expression: concat(['track:', TrackId])
            language: jmespath
    ```

8. Data from a normalized source database can be denormalized with RDI. For example, here is the denormalization job from this lab:

    ```yaml
    source:
    server_name: chinook
    schema: public
    table: InvoiceLine
    output:
    - uses: redis.write
      with:
        nest: # cannot co-exist with other parameters such as 'key'
          parent:
            table: Invoice
          nesting_key: InvoiceLineId # cannot be composite
          parent_key: InvoiceId # cannot be composite
          path: $.InvoiceLineItems # path must start from document root ($)
          structure: map # only map supported for now
        on_update: merge # only merge supported for now
        data_type: json # only json supported for now
    ```

9. Data from a source database can be transformed into various Redis data structures, including hashes, JSON, streams, strings, sets, and sorted sets

</details>
<details>
<summary>Observability and troubleshooting</summary>
1. There are three general categories for monitoring RDI: RDI metrics, logs, and the Redis support package
2. RDI metrics can be collected from two areas:
    - The collector, which hosts metrics around pipeline state, data flow counters, and processing performance. The collector metrics can be found at the following endpoint `https://<RDI_HOST>/metrics/collector-source`
    - The stream processor, which hosts metrics for the two main phases of RDI pipelines: initial cache loading and the change streaming. The stream processor metrics can be found at the following endpoint `https://<RDI_HOST>/metrics/rdi`
3. RDI stores logs in the host VM file system at `/opt/rdi/logs`
4. Redis Insight hosts a variety of data that makes it easy to observe RDI.

    ![](/doc/assets/rdi-insights.svg)

5. There are a variety of issues that can occur when working with RDI.
    - If there is an issue with installation, use the `DEBUG` option in your install process
    - If there is an issue with transformed or corrupted data, check the Data Streams section of the RDI pipeline dashboard. You can also use the `redis-di get-rejected` command and the more general `redis-di status` command.
    - If RDI can't write or connect to a database, it will use a method called "back pressure" to get data from the source less often. This'll help it save space. RDI will keep attempting to write the changes and will also attempt to reconnect to it.
</details>
